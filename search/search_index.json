{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"NPS Hikes","text":"<p>A Python project for collecting, validating, and analyzing hiking trail data from U.S. National Parks. The project combines data from the National Park Service API, OpenStreetMap, and the USGS to build a PostGIS database of park boundaries and hiking trails, queryable through a REST API.</p>"},{"location":"#live-demo","title":"Live demo","text":"<p>You can find a live instance of the API at seanangio-nps-hikes.onrender.com. There you can browse the interactive Swagger UI and query park and trail data without any local setup.</p> <p>Note: The live demo runs on a free tier and may take 30-60 seconds to respond on the first request while the server wakes up. Visualization endpoints (maps, elevation charts, 3D trails) are only available with a local deployment.</p>"},{"location":"#project-overview","title":"Project overview","text":"<ul> <li>Collect park metadata and boundaries from the NPS API.</li> <li>Extract hiking trails from OpenStreetMap and The National Map.</li> <li>Match personal hiking locations to trail geometries.</li> <li>Explore parks and trails through a FastAPI REST API.</li> </ul>"},{"location":"#project-structure","title":"Project structure","text":"<pre><code>nps-hikes/\n\u251c\u2500\u2500 api/                       # FastAPI REST API\n\u2502   \u251c\u2500\u2500 main.py                        # API endpoints and application\n\u2502   \u251c\u2500\u2500 models.py                      # Pydantic response models\n\u2502   \u251c\u2500\u2500 queries.py                     # Database query functions\n\u2502   \u2514\u2500\u2500 database.py                    # Database connection management\n\u251c\u2500\u2500 scripts/                   # Data collection and processing scripts\n\u2502   \u251c\u2500\u2500 collectors/            # Data collection from external sources\n\u2502   \u251c\u2500\u2500 processors/            # Data processing and analysis\n\u2502   \u251c\u2500\u2500 database/              # Database management utilities\n\u2502   \u2514\u2500\u2500 orchestrator.py        # Complete pipeline orchestration\n\u251c\u2500\u2500 config/                    # Configuration and settings\n\u251c\u2500\u2500 profiling/                 # Data quality analysis modules\n\u251c\u2500\u2500 tests/                     # Test suite\n\u251c\u2500\u2500 docs/                      # Documentation (this site)\n\u2514\u2500\u2500 utils/                     # Logging and utility functions\n</code></pre>"},{"location":"#data-collection-pipeline","title":"Data collection pipeline","text":"<p>The pipeline runs six steps in order:</p> Step What it does Data source 1. NPS Data Collection Park metadata, coordinates, and boundary polygons NPS API 2. OSM Trails Collection Hiking trails within park boundaries OpenStreetMap 3. TNM Trails Collection Official trail data within park boundaries The National Map 4. GMaps Import Hiking locations from Google My Maps KML files KML files in <code>raw_data/gmaps/</code> 5. Trail Matching Matches GMaps locations to TNM or OSM trail geometries Internal 6. Elevation Collection Elevation profiles for matched trails USGS EPQS <p>The pipeline is resumable: each collector skips parks or trails that already have data in the database. If something interrupts a run, re-running picks up where it left off.</p>"},{"location":"api-tutorial/","title":"Using the API","text":"<p>This tutorial walks through the API's capabilities, starting with a broad overview of your parks and progressively narrowing down to individual trail visualizations. By the end, you'll know how to query parks and trails, generate visualizations, and build a 3D elevation profile for a specific trail.</p> <p>The tutorial assumes you've completed the Getting Started guide, run the full data collection pipeline, and have both Docker services running.</p> <p>The examples below use <code>curl</code> with <code>python3 -m json.tool</code> for pretty-printed output. You can also paste the URLs directly into your browser, which is especially useful for the visualization endpoints that return images and interactive HTML.</p> <p>Tip: Don't want to set up locally? You can try the data query endpoints on the live demo. In the examples below, replace <code>http://localhost:8000</code> with <code>https://seanangio-nps-hikes.onrender.com</code>. Note that visualization endpoints (maps, elevation charts, etc.) are only available locally.</p>"},{"location":"api-tutorial/#orient-yourself","title":"Orient yourself","text":"<p>First, confirm that the API is running and that the database is connected:</p> <pre><code>curl http://localhost:8000/health | python3 -m json.tool\n</code></pre> <p>You should see:</p> <pre><code>{\n    \"status\": \"healthy\",\n    \"database\": \"connected\"\n}\n</code></pre> <p>The root endpoint lists all available endpoints:</p> <pre><code>curl http://localhost:8000/ | python3 -m json.tool\n</code></pre> <pre><code>{\n    \"name\": \"NPS Trails API\",\n    \"version\": \"1.0.0\",\n    \"description\": \"Query National Park trail data from OpenStreetMap and The National Map\",\n    \"documentation\": {\n        \"swagger_ui\": \"/docs\",\n        \"redoc\": \"/redoc\",\n        \"openapi_json\": \"/openapi.json\"\n    },\n    \"endpoints\": {\n        \"parks\": \"/parks\",\n        \"trails\": \"/trails\",\n        \"us_static_park_map\": \"/parks/viz/us-static-park-map\",\n        \"us_interactive_park_map\": \"/parks/viz/us-interactive-park-map\",\n        \"static_map\": \"/parks/{park_code}/viz/static-map\",\n        \"elevation_matrix\": \"/parks/{park_code}/viz/elevation-matrix\",\n        \"trail_3d_viz\": \"/parks/{park_code}/trails/{trail_slug}/viz/3d\",\n        \"health_check\": \"/health\"\n    }\n}\n</code></pre> <p>Tip: For a full interactive reference, open http://localhost:8000/docs in your browser. The Swagger UI lets you try every endpoint, inspect request/response schemas, and experiment with query parameters.</p>"},{"location":"api-tutorial/#browse-your-parks","title":"Browse your parks","text":"<p>The <code>/parks</code> endpoint returns a <code>park_count</code>, a <code>visited_count</code>, and a <code>parks</code> array. Each park includes its 4-letter code, name, state, coordinates, and NPS URL.</p> <pre><code>curl http://localhost:8000/parks | python3 -m json.tool\n</code></pre> <p>To see only the parks from your visit log, add the <code>visited</code> filter:</p> <pre><code>curl \"http://localhost:8000/parks?visited=true\" | python3 -m json.tool\n</code></pre> <pre><code>{\n    \"park_count\": 3,\n    \"visited_count\": 3,\n    \"parks\": [\n        {\n            \"park_code\": \"acad\",\n            \"park_name\": \"Acadia National Park\",\n            \"full_name\": \"Acadia National Park\",\n            \"designation\": \"National Park\",\n            \"states\": \"ME\",\n            \"latitude\": 44.3386,\n            \"longitude\": -68.2733,\n            \"url\": \"https://www.nps.gov/acad/index.htm\",\n            \"visit_month\": \"Oct\",\n            \"visit_year\": 2024\n        },\n        ...\n    ]\n}\n</code></pre> <p>Visited parks include <code>visit_month</code> and <code>visit_year</code> from your visit log. Flip the filter to see your park bucket list:</p> <pre><code>curl \"http://localhost:8000/parks?visited=false\" | python3 -m json.tool\n</code></pre> <p>For richer responses, add <code>include_description=true</code> to include the full NPS description for each park:</p> <pre><code>curl \"http://localhost:8000/parks?visited=true&amp;include_description=true\" | python3 -m json.tool\n</code></pre>"},{"location":"api-tutorial/#see-the-big-picture","title":"See the big picture","text":"<p>The API can serve map visualizations of all your parks. These endpoints return pre-generated images, so you need to generate them first. Run the US park map profiling module:</p> <pre><code>POSTGRES_HOST=localhost POSTGRES_PORT=5433 python profiling/orchestrator.py us_park_map\n</code></pre> <p>Now open the static map in your browser:</p> <pre><code>http://localhost:8000/parks/viz/us-static-park-map\n</code></pre> <p>This returns a PNG image showing all national parks on a US map with Alaska and Hawaii insets. Parks are color-coded by your visit log.</p> <p></p> <p>For an interactive version with hover tooltips and park boundaries, open:</p> <pre><code>http://localhost:8000/parks/viz/us-interactive-park-map\n</code></pre> <p>This returns an HTML page with a zoomable Plotly map. Hover over any park to see its name, state, and visit status. You can zoom in to see (rough) park boundary outlines.</p>"},{"location":"api-tutorial/#park-level-visualizations","title":"Park-level visualizations","text":"<p>The API also serves per-park trail maps and elevation charts. Generate them with:</p> <pre><code>POSTGRES_HOST=localhost POSTGRES_PORT=5433 python profiling/orchestrator.py visualization usgs_elevation_viz\n</code></pre> <p>Note: The <code>visualization</code> module generates static trail maps for every park with trail data. The <code>usgs_elevation_viz</code> module generates elevation matrices for parks with elevation data. Depending on how many parks have data, this may take a few minutes.</p>"},{"location":"api-tutorial/#static-trail-map","title":"Static trail map","text":"<p>Open a trail map for a specific park using its 4-letter code:</p> <pre><code>http://localhost:8000/parks/acad/viz/static-map\n</code></pre> <p>This returns a PNG image showing the park boundary and all collected trails. Trails are color-coded by data source: blue for OpenStreetMap, orange for The National Map. Purple points mark hiking locations imported from your KML files.</p> <p></p> <p>Tip: Replace <code>acad</code> with any park code to see its trail map (for example, <code>/parks/yose/viz/static-map</code> for Yosemite).</p>"},{"location":"api-tutorial/#elevation-matrix","title":"Elevation matrix","text":"<p>For parks where you have matched trails with elevation data, there's an elevation profile matrix:</p> <pre><code>http://localhost:8000/parks/acad/viz/elevation-matrix\n</code></pre> <p>This returns a PNG grid of elevation charts, one per matched trail. Each chart shows distance on the x-axis and elevation on the y-axis, giving you a quick visual comparison across trails.</p> <p></p>"},{"location":"api-tutorial/#explore-trails","title":"Explore trails","text":"<p>The <code>/trails</code> endpoint returns trail data from both The National Map (TNM) and OpenStreetMap (OSM). Start by querying trails for a specific park:</p> <pre><code>curl \"http://localhost:8000/trails?park_code=acad\" | python3 -m json.tool\n</code></pre> <p>The response includes a <code>trail_count</code>, <code>total_miles</code>, and a <code>trails</code> array. Each trail includes:</p> Field Description <code>trail_id</code> Unique identifier (from TNM or OSM) <code>trail_name</code> Trail name <code>source</code> Data source (<code>TNM</code> or <code>OSM</code>) <code>length_miles</code> Trail length in miles <code>hiked</code> Whether you've hiked this trail (matched from your KML files) <code>viz_3d_available</code> Whether a 3D visualization can be generated <code>viz_3d_slug</code> URL slug for the 3D endpoint (if available) <p>When querying a single park, results are sorted by length (longest first). When querying across multiple parks, results are sorted by park code and trail name.</p> <p>Note: The API deduplicates trails that appear in both data sources. When a TNM trail and an OSM trail in the same park share more than 70% name similarity, the API keeps the TNM version. This avoids double-counting while preferring the more detailed TNM data.</p>"},{"location":"api-tutorial/#filter-by-data-source","title":"Filter by data source","text":"<p>Compare what each data source provides:</p> <pre><code>curl \"http://localhost:8000/trails?park_code=acad&amp;source=TNM\" | python3 -m json.tool\ncurl \"http://localhost:8000/trails?park_code=acad&amp;source=OSM\" | python3 -m json.tool\n</code></pre>"},{"location":"api-tutorial/#filter-by-hiked-status","title":"Filter by hiked status","text":"<p>See which trails you've hiked based on your KML data:</p> <pre><code>curl \"http://localhost:8000/trails?hiked=true\" | python3 -m json.tool\n</code></pre> <p>Or find trails you haven't hiked yet in a specific park:</p> <pre><code>curl \"http://localhost:8000/trails?park_code=acad&amp;hiked=false\" | python3 -m json.tool\n</code></pre>"},{"location":"api-tutorial/#filter-by-length","title":"Filter by length","text":"<p>Find trails within a specific length range:</p> <pre><code>curl \"http://localhost:8000/trails?min_length=5&amp;max_length=15\" | python3 -m json.tool\n</code></pre>"},{"location":"api-tutorial/#filter-by-state","title":"Filter by state","text":"<p>Query trails across all parks in a state:</p> <pre><code>curl \"http://localhost:8000/trails?state=CA\" | python3 -m json.tool\n</code></pre> <p>You can combine multiple states by repeating the parameter:</p> <pre><code>curl \"http://localhost:8000/trails?state=CA&amp;state=UT\" | python3 -m json.tool\n</code></pre> <p>Note: State parameters surface state data as returned from the NPS API, rather than geographic validation. Keep this in mind when querying trails in parks that span multiple states (Yellowstone for example).</p>"},{"location":"api-tutorial/#the-3d-trail-journey","title":"The 3D trail journey","text":"<p>The most detailed visualization in the API is an interactive 3D elevation profile for individual trails. Getting there takes a few steps because you need to discover which trails have elevation data and find their URL slugs.</p>"},{"location":"api-tutorial/#find-trails-with-3d-data","title":"Find trails with 3D data","text":"<p>Start by filtering for trails with 3D visualizations available:</p> <pre><code>curl \"http://localhost:8000/trails?park_code=acad&amp;viz_3d=true\" | python3 -m json.tool\n</code></pre> <p>In the response, look for the <code>viz_3d_slug</code> field on each trail:</p> <pre><code>{\n    \"trail_count\": 2,\n    \"total_miles\": 5.8,\n    \"trails\": [\n        {\n            \"trail_id\": \"123456\",\n            \"trail_name\": \"Jordan Pond Path\",\n            \"park_code\": \"acad\",\n            \"source\": \"TNM\",\n            \"length_miles\": 3.4,\n            \"hiked\": true,\n            \"viz_3d_available\": true,\n            \"viz_3d_slug\": \"jordan_pond_path\"\n        },\n        ...\n    ]\n}\n</code></pre> <p>The <code>viz_3d_slug</code> value is what you need for the next step.</p>"},{"location":"api-tutorial/#open-the-3d-visualization","title":"Open the 3D visualization","text":"<p>Build the URL using the park code and trail slug, and open it in your browser:</p> <pre><code>http://localhost:8000/parks/acad/trails/jordan_pond_path/viz/3d\n</code></pre> <p>This opens an interactive 3D Plotly visualization. You can rotate, zoom, and pan the trail. The trail is color-coded by elevation using a terrain gradient (brown at lower elevations, cream in the middle, and blue-green at higher elevations). Hover over any point to see its distance along the trail and elevation.</p> <p></p>"},{"location":"api-tutorial/#adjust-the-vertical-scale","title":"Adjust the vertical scale","text":"<p>By default, the z-axis is exaggerated by a factor of 5 to make elevation changes more visible. You can adjust this with the <code>z_scale</code> parameter (range: 1 to 20):</p> <pre><code>http://localhost:8000/parks/acad/trails/jordan_pond_path/viz/3d?z_scale=10\n</code></pre> <p>A higher value makes elevation changes more dramatic. A value of 1 shows true proportions, which can make trails appear nearly flat.</p>"},{"location":"api-tutorial/#what-makes-a-trail-eligible","title":"What makes a trail eligible","text":"<p>Not every trail has a 3D visualization. A trail needs elevation data, which requires this chain:</p> <ol> <li>You added a hiking point near the trail in your KML files.</li> <li>The trail matching step matched that point to a trail geometry.</li> <li>The elevation collection step sampled points along the trail and queried the USGS for elevations.</li> </ol> <p>This is why your KML files matter. They determine which trails get elevation data and 3D visualizations.</p> <p>Note: Unlike the park maps and elevation matrices from the previous sections, 3D trail visualizations don't require the profiling step. The API generates them on-demand from the elevation data already in the database.</p>"},{"location":"api-tutorial/#combine-and-discover","title":"Combine and discover","text":"<p>All the filters on the <code>/trails</code> endpoint can be combined. Here are a few queries that answer real questions:</p> <p>What long trails haven't I hiked in Utah?</p> <pre><code>curl \"http://localhost:8000/trails?state=UT&amp;hiked=false&amp;min_length=5\" | python3 -m json.tool\n</code></pre> <p>Which of my hiked trails have 3D visualizations?</p> <pre><code>curl \"http://localhost:8000/trails?hiked=true&amp;viz_3d=true\" | python3 -m json.tool\n</code></pre> <p>From there, pick any trail with a <code>viz_3d_slug</code> and open its 3D visualization in your browser.</p> <p>Tip: You can also use Python's <code>requests</code> library, the Swagger UI at <code>/docs</code>, or any HTTP client to query these endpoints.</p>"},{"location":"data/","title":"Data Sources and Schema","text":""},{"location":"data/#external-sources","title":"External sources","text":"<p>The project collects data from the following APIs.</p>"},{"location":"data/#national-park-service","title":"National Park Service","text":"<p>The NPS API requires a free API key, and provides park metadata and boundary geometries.</p> <ul> <li>The collector queries the <code>/parks</code> endpoint for park names, codes, designations, coordinates, descriptions, and contact information, filtering for National Park designations.</li> <li>A second pass fetches GeoJSON boundary polygons from the <code>/mapdata/parkboundaries/{park_code}</code> endpoint. The collector standardizes boundaries to MultiPolygon format with bounding boxes calculated for downstream spatial queries.</li> </ul>"},{"location":"data/#openstreetmap","title":"OpenStreetMap","text":"<p>The pipeline collects trail geometries from OpenStreetMap via the Overpass API using the <code>osmnx</code> library. Note that the collector:</p> <ul> <li>Queries for paths and footways (<code>highway=path|footway</code>) within each park's boundary polygon.</li> <li>Retains named trails only.</li> <li>Aggregates segments sharing the same name into single MultiLineString records.</li> <li>Calculates trail lengths in miles using a projected coordinate system <code>EPSG:5070</code>.</li> <li>Clips trails to the park boundary before storage.</li> </ul>"},{"location":"data/#the-national-map","title":"The National Map","text":"<p>The National Map provides official USGS trail data through an ArcGIS REST endpoint. The collector queries using each park's bounding box and returns trail geometries along with detailed attributes: trail type, trail number, length, and use flags (hiker, bicycle, pack/saddle, cross-country ski, etc.). Like OSM trails, the collector aggregates segments by name, clips to the park boundary, and recalculates lengths in the projected CRS.</p>"},{"location":"data/#usgs-elevation-point-query-service","title":"USGS Elevation Point Query Service","text":"<p>The USGS EPQS returns elevation in meters for individual latitude/longitude coordinates. The collector samples points along matched trail geometries at regular intervals (default 50 meters) and queries the service for each point. The collector caches results locally to avoid redundant API calls. A three-stage validation pipeline checks API responses, individual point values, and the complete elevation profile. It treates the USGS sentinel value of <code>-1,000,000</code> as missing data.</p>"},{"location":"data/#coordinate-systems","title":"Coordinate systems","text":"<p>All geographic data uses <code>EPSG:4326 (WGS84)</code>. Length calculations use <code>EPSG:5070 (NAD83/Conus Albers)</code> for accurate distance measurements in meters.</p>"},{"location":"data/#internal-sources","title":"Internal sources","text":"<p>For the following two sources, you can replace the author's files with your own.</p>"},{"location":"data/#park-visit-log","title":"Park visit log","text":"<p>A CSV file (<code>raw_data/park_visit_log.csv</code>) recording which national parks you've visited, with park name, month, and year. The NPS collector uses this to tag parks as visited or unvisited, enabling filtered queries through the API. See Park visit log for formatting details.</p>"},{"location":"data/#google-my-maps-kml-files","title":"Google My Maps KML files","text":"<p>KML files exported from Google My Maps containing named hiking location points, organized into layers by 4-letter park code. The pipeline imports these points, matches them to the nearest trail geometries from TNM or OSM, and then collects elevation profiles for the matched trails. See Google My Maps hiking data for export instructions.</p>"},{"location":"data/#database-schema","title":"Database schema","text":""},{"location":"data/#core-tables","title":"Core tables","text":"<p>The pipeline orchestrator creates the following tables:</p> Table Description parks Park metadata (codes, names, coordinates, descriptions, visit dates) park_boundaries Spatial boundaries as MultiPolygon geometries in WGS84 osm_hikes Aggregated trail geometries from OpenStreetMap (segments with same name combined into MultiLineString) tnm_hikes Trail data from The National Map with detailed trail characteristics gmaps_hiking_locations Google Maps hiking location points with coordinates gmaps_hiking_locations_matched Matched locations with trail correlation results usgs_trail_elevations Elevation profile data for matched trails"},{"location":"data/#key-features","title":"Key features","text":"<ul> <li>Spatial indexing with PostGIS GIST indexes for performance</li> <li>Foreign key relationships for data integrity across tables</li> <li>Composite primary keys for trail uniqueness (<code>park_code</code> + <code>osm_id</code>)</li> <li>Coordinate validation with proper range constraints</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks through locally setting up this NPS hiking project from scratch. By the end, you'll have a PostGIS database with all 63 US national parks and hundreds of hiking trails, queryable through an interactive API.</p> <p>Tip: Instead of a local setup, you can also explore a live demo of the Swagger UI docs at seanangio-nps-hikes.onrender.com/docs. To query the visualization endpoints however, continue with the local setup instructions here.</p>"},{"location":"getting-started/#step-0-prerequisites","title":"Step 0: Prerequisites","text":"<p>Before you begin, make sure you have the following:</p> <ul> <li>Docker Desktop: Install Docker Desktop for your operating system. Docker runs the database and API in containers so you don't need to install PostgreSQL or PostGIS locally.</li> <li>Python 3.12+: The data collection pipeline runs on your local machine. Check your version with <code>python3 --version</code>. If you need to install or upgrade, see python.org.</li> <li>Git: Install Git for your operating system to clone the repository.</li> <li>An NPS API key: Free to sign up at the NPS Developer Portal. You should receive a key by email within minutes.</li> </ul>"},{"location":"getting-started/#step-1-clone-the-repository","title":"Step 1: Clone the repository","text":"<p>Start by cloning the repository.</p> <pre><code>git clone https://github.com/seanangio/nps-hikes.git\ncd nps-hikes\n</code></pre>"},{"location":"getting-started/#step-2-set-up-a-python-environment","title":"Step 2: Set up a Python environment","text":"<p>Create a virtual environment, and install the project dependencies. You need them for the data collection pipeline, which runs outside of Docker.</p> <pre><code>python3.12 -m venv .venv\nsource .venv/bin/activate    # On Windows: .venv\\Scripts\\activate\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/#step-3-configure-environment-variables","title":"Step 3: Configure environment variables","text":"<p>Copy the example environment file, and fill in your credentials:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Open <code>.env</code> in your editor. You need to set two values:</p> <pre><code># Required: your NPS API key from Step 0\nNPS_API_KEY=your_actual_api_key\n\n# Required: choose any password for the Docker database\nPOSTGRES_PASSWORD=choose_a_password\n</code></pre> <p>The remaining defaults work as-is for the Docker setup:</p> <pre><code># No changes required for these defaults\nPOSTGRES_USER=postgres\nNPS_USER_EMAIL=your_email@example.com\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_DB=nps_hikes_db\n</code></pre> <p>How the project uses the <code>.env</code> file: Docker Compose reads it automatically to configure the database container. The Python scripts also read it (via <code>python-dotenv</code>) for API credentials and database connections.</p>"},{"location":"getting-started/#step-4-personalize-the-raw-data","title":"Step 4: Personalize the raw data","text":"<p>The repository includes two types of sample raw data that you can substitute with your own.</p>"},{"location":"getting-started/#park-visit-log","title":"Park visit log","text":"<p>The file <code>raw_data/park_visit_log.csv</code> records which parks you've visited:</p> <pre><code>park_name,month,year\nYosemite,July,2023\nGrand Canyon,March,2024\nAcadia,Oct,2024\n</code></pre> <p>Edit this file with your own visits. The <code>park_name</code> column requires the common short name (for example, \"Yosemite\" not \"Yosemite National Park\"). The collector appends \"National Park\" automatically, so \"Yosemite\" becomes \"Yosemite National Park\" and matches directly.</p> <p>For parks with other designations like \"National Park &amp; Preserve,\" the collector falls back to substring matching. For example, \"Denali\" doesn't match \"Denali National Park &amp; Preserve\" exactly, but the pipeline finds a match because the string \"Denali\" is a substring of the official name.</p> <p>Make sure your entry is an exact substring of the official name. For example, use \"Redwood\" (not \"Redwoods\") for Redwood National and State Parks.</p> <p>Tip: When you test the pipeline below in Step 6, it processes the first NPS park alphabetically (Acadia). If you include Acadia in your visit log, you'll have a visited park with trail data to explore.</p>"},{"location":"getting-started/#google-my-maps-hiking-data","title":"Google My Maps hiking data","text":"<p>The <code>raw_data/gmaps/</code> directory contains KML files with hiking locations exported from Google My Maps. The pipeline uses these named points to match hiking locations to trail geometries, and then collects elevation data for the matched trails. This enables personalized trail matching, hiked/unhiked filtering, and 3D trail visualizations with elevation profiles.</p> <p>Tip: The repository includes sample KML files from the author's hikes. You can substitute your own files following the instructions below. Otherwise, leave the samples as-is, and skip ahead to Step 5.</p>"},{"location":"getting-started/#how-it-works","title":"How it works","text":"<p>The pipeline processes every <code>.kml</code> file in the <code>raw_data/gmaps/</code> directory. Inside each KML file, it looks for folders (layers) named for 4-letter park codes, and reads the placemarks within them. A single Google My Maps KML file can contain up to ten layers (one per park).</p> <p>Finding park codes: You can find the 4-letter abbreviation for each park on the NPS website in each park's URL. Once the API is running, they're also available at <code>http://localhost:8000/parks</code>.</p>"},{"location":"getting-started/#create-your-hiking-maps","title":"Create your hiking maps","text":"<p>In Google My Maps, create one or more maps for your hikes:</p> <ol> <li>Add a layer for each park, named according to the 4-letter park code (for example, <code>zion</code>).</li> <li>Add placemarks to each layer for the trails or locations you've hiked.</li> </ol>"},{"location":"getting-started/#export-and-add-kml-files","title":"Export and add KML files","text":"<p>Export each map as a KML file, and save the files to <code>raw_data/gmaps/</code>:</p> <pre><code>raw_data/gmaps/\n\u251c\u2500\u2500 nps_points_west.kml    # could contain layers: zion, yose, grca, ...\n\u2514\u2500\u2500 nps_points_east.kml    # could contain layers: acad, shen, grsm, ...\n</code></pre>"},{"location":"getting-started/#step-5-start-the-docker-services","title":"Step 5: Start the Docker services","text":"<p>Make sure Docker Desktop is running. Then launch the database and API containers:</p> <pre><code>docker compose up --build -d\n</code></pre> <p>This starts two services:</p> Service Port Description <code>db</code> 5433 PostGIS database (mapped to 5433 to avoid conflicts with any local PostgreSQL) <code>api</code> 8000 FastAPI REST API <p>Tip: The first run may take a few minutes while Docker downloads the base images. Subsequent runs are much faster.</p> <p>On first startup, the database container automatically creates the required PostGIS and pg_trgm extensions and runs all schema migrations. You can verify the services are running:</p> <pre><code>docker compose ps\n</code></pre> <p>You should see both <code>db</code> and <code>api</code> with a status of \"Up\" (the database should show \"healthy\").</p> <p>Note: The database uses port 5433 on your machine, not the standard 5432. This is intentional to avoid conflicts if you have PostgreSQL installed locally.</p>"},{"location":"getting-started/#step-6-run-the-data-collection-pipeline","title":"Step 6: Run the data collection pipeline","text":"<p>Next, populate the database with park and trail data. The pipeline runs on your local machine and writes to the Docker database.</p> <p>Since the Docker database is on port 5433, override the port when running the pipeline:</p> <pre><code>POSTGRES_HOST=localhost POSTGRES_PORT=5433 python scripts/orchestrator.py --write-db --test-limit 1\n</code></pre> <p>The <code>--test-limit 1</code> flag processes only one park, so you can verify it works before committing to the full run. Due to the elevation data collection step, this test run may take approximately 10 minutes.</p> <p>The pipeline runs six steps in order:</p> Step What it does Data source 1. NPS Data Collection Park metadata, coordinates, and boundary polygons NPS API 2. OSM Trails Collection Hiking trails within park boundaries OpenStreetMap 3. TNM Trails Collection Official trail data within park boundaries The National Map 4. GMaps Import Hiking locations from Google My Maps KML files KML files in <code>raw_data/gmaps/</code> 5. Trail Matching Matches GMaps locations to TNM or OSM trail geometries Internal 6. Elevation Collection Elevation profiles for matched trails USGS EPQS"},{"location":"getting-started/#verify-the-test-run","title":"Verify the test run","text":"<p>First, confirm that the pipeline created and populated the tables by querying the database directly:</p> <pre><code>docker compose exec db bash -c 'psql -U \"$POSTGRES_USER\" -d \"$POSTGRES_DB\" -c \"SELECT park_code, park_name FROM parks;\"'\n</code></pre> <p>Tip: You should see the park code and name of parks collected (one if you used <code>--test-limit 1</code>). This runs <code>psql</code> inside the already-running database container, so you don't need PostgreSQL installed locally.</p> <p>You can also verify through the API:</p> <pre><code>curl http://localhost:8000/parks | python3 -m json.tool\n</code></pre> <p>You should see a JSON response with <code>park_count</code> showing the number of parks collected and a <code>parks</code> array with details for each one.</p>"},{"location":"getting-started/#run-the-full-pipeline","title":"Run the full pipeline","text":"<p>Once you've confirmed the test run works, collect data for all 63<sup>1</sup> national parks:</p> <pre><code>POSTGRES_HOST=localhost POSTGRES_PORT=5433 python scripts/orchestrator.py --write-db\n</code></pre> <p>This takes longer. If using the author's files, expect more than 2 hours for the full run. The main bottleneck is the elevation collection step, which queries the USGS EPQS API for sampled points along each matched trail (one request per point, with a rate limit delay between calls). The more trails matched from your KML files, the longer this step takes.</p> <p>The pipeline is resumable: with <code>--write-db</code>, each collector skips parks or trails that already have data in the database, and the elevation collector also maintains a persistent cache of individual elevation lookups. If something interrupts a run, re-running the same command picks up roughly where it left off. To force a full re-collection, pass <code>--force-refresh</code>.</p> <p>Tip: The pipeline is fail-fast. If a step fails, check <code>logs/orchestrator.log</code> for details. You can also run individual collectors directly for debugging (see the README for individual component commands).</p>"},{"location":"getting-started/#step-7-explore-your-data","title":"Step 7: Explore your data","text":"<p>With the pipeline complete, you now have a database full of national park and trail data. The API should be running at <code>http://localhost:8000</code>.</p>"},{"location":"getting-started/#interactive-api-documentation","title":"Interactive API documentation","text":"<p>Open http://localhost:8000/docs in your browser to access the Swagger UI. This interactive interface lets you try every endpoint, see request/response schemas, and experiment with query parameters.</p>"},{"location":"getting-started/#quick-examples","title":"Quick examples","text":"Description URL Browse all parks <code>http://localhost:8000/parks</code> Filter to parks you've visited <code>http://localhost:8000/parks?visited=true</code> See all trails for a specific park <code>http://localhost:8000/parks/yose/trails</code> Find long trails across all parks <code>http://localhost:8000/trails?min_length=10</code> Filter by state <code>http://localhost:8000/trails?state=CA</code> <p>Note: The data endpoints (<code>/parks</code>, <code>/trails</code>) work immediately after the pipeline. The visualization endpoints (park maps, trail maps, elevation charts) require an additional generation step covered in the API Tutorial.</p>"},{"location":"getting-started/#stopping-and-restarting","title":"Stopping and restarting","text":"<p>Here are a few handy commands for stopping and restarting the Docker services.</p> <p>Stop the services (preserving the data):</p> <pre><code>docker compose down\n</code></pre> <p>Restart later (no rebuild needed unless code changed):</p> <pre><code>docker compose up -d\n</code></pre> <p>Start fresh (removes all database data):</p> <pre><code>docker compose down -v\n</code></pre>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#set-postgres_password-in-env","title":"\"Set POSTGRES_PASSWORD in .env\"","text":"<p>Docker Compose requires setting a <code>POSTGRES_PASSWORD</code>. Make sure your <code>.env</code> file exists in the project root and contains a <code>POSTGRES_PASSWORD</code> value.</p>"},{"location":"getting-started/#pipeline-cant-connect-to-the-database","title":"Pipeline can't connect to the database","text":"<p>When running the pipeline from your local machine against the Docker database, make sure you're using port 5433:</p> <pre><code>POSTGRES_HOST=localhost POSTGRES_PORT=5433 python scripts/orchestrator.py --write-db\n</code></pre>"},{"location":"getting-started/#visit-log-file-not-found","title":"\"Visit log file not found\"","text":"<p>The NPS collector expects a file at <code>raw_data/park_visit_log.csv</code>. If you don't have one, create it with just the header row:</p> <pre><code>echo \"park_name,month,year\" &gt; raw_data/park_visit_log.csv\n</code></pre>"},{"location":"getting-started/#docker-containers-wont-start","title":"Docker containers won't start","text":"<p>Make sure Docker Desktop is running, then try rebuilding:</p> <pre><code>docker compose down\ndocker compose up --build\n</code></pre> <p>Check the logs for specific errors:</p> <pre><code>docker compose logs db\ndocker compose logs api\n</code></pre>"},{"location":"getting-started/#api-returns-empty-results-after-pipeline","title":"API returns empty results after pipeline","text":"<p>Verify that the pipeline wrote data to the database:</p> <pre><code>curl http://localhost:8000/health\n</code></pre> <p>The response should show <code>\"database\": \"connected\"</code>. If connected but no data, re-run the pipeline and check <code>logs/orchestrator.log</code> for errors.</p>"},{"location":"getting-started/#next-steps","title":"Next steps","text":"<ul> <li>API Tutorial: A guided tour of the API's query capabilities and visualizations</li> <li>README; Full project documentation including architecture, testing, and data profiling</li> </ul> <ol> <li> <p>The NPS manages Sequoia and Kings Canyon as one park (<code>seki</code>), and so it appears as one entry.\u00a0\u21a9</p> </li> </ol>"}]}